<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cluster Maintenance &mdash; CKA Notes  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/custom.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/clipboard@1/dist/clipboard.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Security" href="security.html" />
    <link rel="prev" title="Application lifecycle management" href="application-lifecycle-management.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> CKA Notes
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The CKABOOK:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="core-concepts.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="scheduling.html">Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging-monitoring.html">Logging and Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="application-lifecycle-management.html">Application lifecycle management</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Cluster Maintenance</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="networking.html">Networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="design-k8s.html">Design Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-k8s-kubeadm.html">Install Kubernetes with kubeadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">troubleshooting in k8s</a></li>
<li class="toctree-l1"><a class="reference internal" href="kodekloud-labs.html">kodekloud labs</a></li>
<li class="toctree-l1"><a class="reference internal" href="killersh-labs.html">killer.sh simulator labs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">CKA Notes</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Cluster Maintenance</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="cluster-maintenance">
<h1>Cluster Maintenance<a class="headerlink" href="#cluster-maintenance" title="Permalink to this headline"></a></h1>
<p>OSupgrades</p>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption"><span class="caption-text">OSupgrades</span><a class="headerlink" href="#id1" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> the node gets shutdown <span class="k">for</span> more than <span class="m">5</span> minits the pods <span class="k">in</span> that node considered as dead.

this is configured on controller-manager as a setting called pod-eviction-timeout
i.e. kube-controller-manager --pod-eviction-timeout<span class="o">=</span>5m0s


<span class="k">in</span> order to safely move all the existing pods to another node... first drain the node

<span class="m">1</span>. kubectl drain node-1 and <span class="k">then</span> perform reboot of the node.
<span class="m">2</span>. After reboot - kubectl uncordon node-1 to schedule the pods on the node-1 again.


kubectl drain node-1  --&gt; it moves all pods to another node and makes unschedulable <span class="k">for</span> pods on that node
kubectl uncordon node-1 --&gt; make node to be schedulable <span class="k">for</span> pods.

kubectl cordon node-1 --&gt; it only makes node unschedulable <span class="k">for</span> pods..it doesnot move existing pods to new node.
</pre></div>
</div>
</div>
<p>OSupgradeslabs</p>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">OSupgradeslabs</span><a class="headerlink" href="#id2" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>root@controlplane:~# kubectl get nodes -o wide
NAME           STATUS   ROLES                  AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready    control-plane,master   6m45s   v1.20.0   <span class="m">10</span>.15.47.11   &lt;none&gt;        Ubuntu <span class="m">18</span>.04.5 LTS   <span class="m">5</span>.4.0-1052-gcp   docker://19.3.0
node01         Ready    &lt;none&gt;                 6m4s    v1.20.0   <span class="m">10</span>.15.47.3    &lt;none&gt;        Ubuntu <span class="m">18</span>.04.5 LTS   <span class="m">5</span>.4.0-1052-gcp   docker://19.3.0
root@controlplane:~# 


root@controlplane:~# kubectl get all    
NAME                        READY   STATUS    RESTARTS   AGE
pod/blue-746c87566d-fppk7   <span class="m">1</span>/1     Running   <span class="m">0</span>          25s
pod/blue-746c87566d-gtss7   <span class="m">1</span>/1     Running   <span class="m">0</span>          25s
pod/blue-746c87566d-vg78s   <span class="m">1</span>/1     Running   <span class="m">0</span>          25s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>   AGE
service/kubernetes   ClusterIP   <span class="m">10</span>.96.0.1    &lt;none&gt;        <span class="m">443</span>/TCP   7m19s

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/blue   <span class="m">3</span>/3     <span class="m">3</span>            <span class="m">3</span>           25s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/blue-746c87566d   <span class="m">3</span>         <span class="m">3</span>         <span class="m">3</span>       25s
root@controlplane:~# 



root@controlplane:~# kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
blue-746c87566d-fppk7   <span class="m">1</span>/1     Running   <span class="m">0</span>          68s   <span class="m">10</span>.244.1.2   node01   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-gtss7   <span class="m">1</span>/1     Running   <span class="m">0</span>          68s   <span class="m">10</span>.244.1.4   node01   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-vg78s   <span class="m">1</span>/1     Running   <span class="m">0</span>          68s   <span class="m">10</span>.244.1.3   node01   &lt;none&gt;           &lt;none&gt;
root@controlplane:~# 


We need to take node01 out <span class="k">for</span> maintenance. Empty the node of all applications and mark it unschedulable.

root@controlplane:~# kubectl drain node01 --ignore-daemonsets
node/node01 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-8s2k4, kube-system/kube-proxy-tx4tg
evicting pod default/blue-746c87566d-vg78s
evicting pod default/blue-746c87566d-gtss7
evicting pod default/blue-746c87566d-fppk7
pod/blue-746c87566d-vg78s evicted
pod/blue-746c87566d-gtss7 evicted
pod/blue-746c87566d-fppk7 evicted
node/node01 evicted
root@controlplane:~# 



root@controlplane:~# kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-746c87566d-7ps97   <span class="m">1</span>/1     Running   <span class="m">0</span>          85s   <span class="m">10</span>.244.0.4   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-9h99k   <span class="m">1</span>/1     Running   <span class="m">0</span>          84s   <span class="m">10</span>.244.0.5   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-frk9j   <span class="m">1</span>/1     Running   <span class="m">0</span>          84s   <span class="m">10</span>.244.0.6   controlplane   &lt;none&gt;           &lt;none&gt;
root@controlplane:~# 



The maintenance tasks have been completed. Configure the node node01 to be schedulable again.

root@controlplane:~# kubectl uncordon node01
node/node01 uncordoned
root@controlplane:~# 


root@controlplane:~# kubectl describe node controlplane <span class="p">|</span> grep -i taint
Taints:             &lt;none&gt;
root@controlplane:~# 


We need to carry out a maintenance activity on node01 again. Try draining the node again using the same <span class="nb">command</span> as before: kubectl drain node01 --ignore-daemonsets


root@controlplane:~# kubectl drain node01 --ignore-daemonsets
node/node01 already cordoned
error: unable to drain node <span class="s2">&quot;node01&quot;</span>, aborting command...

There are pending nodes to be drained:
 node01
error: cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet <span class="o">(</span>use --force to override<span class="o">)</span>: default/hr-app
root@controlplane:~# 


root@controlplane:~# kubectl get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/blue-746c87566d-7ps97   <span class="m">1</span>/1     Running   <span class="m">0</span>          6m53s
pod/blue-746c87566d-9h99k   <span class="m">1</span>/1     Running   <span class="m">0</span>          6m52s
pod/blue-746c87566d-frk9j   <span class="m">1</span>/1     Running   <span class="m">0</span>          6m52s
pod/hr-app                  <span class="m">1</span>/1     Running   <span class="m">0</span>          2m37s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>   AGE
service/kubernetes   ClusterIP   <span class="m">10</span>.96.0.1    &lt;none&gt;        <span class="m">443</span>/TCP   16m

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/blue   <span class="m">3</span>/3     <span class="m">3</span>            <span class="m">3</span>           9m13s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/blue-746c87566d   <span class="m">3</span>         <span class="m">3</span>         <span class="m">3</span>       9m13s
root@controlplane:~# 


a single pod scheduled on node01 which is not part of a replicaset.
The drain <span class="nb">command</span> will not work <span class="k">in</span> this <span class="k">case</span>. To forcefully drain the node we now have to use the --force flag.



Mark node01 as unschedulable so that no new pods are scheduled on this node.
Make sure that hr-app is not affected

root@controlplane:~# kubectl cordon node01
node/node01 cordoned
root@controlplane:~#


root@controlplane:~# kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-746c87566d-7ps97     <span class="m">1</span>/1     Running   <span class="m">0</span>          10m     <span class="m">10</span>.244.0.4   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-9h99k     <span class="m">1</span>/1     Running   <span class="m">0</span>          10m     <span class="m">10</span>.244.0.5   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-frk9j     <span class="m">1</span>/1     Running   <span class="m">0</span>          10m     <span class="m">10</span>.244.0.6   controlplane   &lt;none&gt;           &lt;none&gt;
hr-app-76d475c57d-j7fkq   <span class="m">1</span>/1     Running   <span class="m">0</span>          2m14s   <span class="m">10</span>.244.1.6   node01         &lt;none&gt;           &lt;none&gt;
root@controlplane:~# 


root@controlplane:~# kubectl create deployment bkapp --image<span class="o">=</span>nginx --replicas<span class="o">=</span><span class="m">5</span> 
deployment.apps/bkapp created
root@controlplane:~# 



root@controlplane:~# kubectl get pods -o wide
NAME                      READY   STATUS              RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
bkapp-765748f886-6gl57    <span class="m">0</span>/1     ContainerCreating   <span class="m">0</span>          34s     &lt;none&gt;       controlplane   &lt;none&gt;           &lt;none&gt;
bkapp-765748f886-cn8kb    <span class="m">0</span>/1     ContainerCreating   <span class="m">0</span>          33s     &lt;none&gt;       controlplane   &lt;none&gt;           &lt;none&gt;
bkapp-765748f886-cndct    <span class="m">0</span>/1     ContainerCreating   <span class="m">0</span>          33s     &lt;none&gt;       controlplane   &lt;none&gt;           &lt;none&gt;
bkapp-765748f886-dtc6h    <span class="m">0</span>/1     ContainerCreating   <span class="m">0</span>          33s     &lt;none&gt;       controlplane   &lt;none&gt;           &lt;none&gt;
bkapp-765748f886-sfr2d    <span class="m">0</span>/1     ContainerCreating   <span class="m">0</span>          33s     &lt;none&gt;       controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-7ps97     <span class="m">1</span>/1     Running             <span class="m">0</span>          11m     <span class="m">10</span>.244.0.4   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-9h99k     <span class="m">1</span>/1     Running             <span class="m">0</span>          11m     <span class="m">10</span>.244.0.5   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-frk9j     <span class="m">1</span>/1     Running             <span class="m">0</span>          11m     <span class="m">10</span>.244.0.6   controlplane   &lt;none&gt;           &lt;none&gt;
hr-app-76d475c57d-j7fkq   <span class="m">1</span>/1     Running             <span class="m">0</span>          3m29s   <span class="m">10</span>.244.1.6   node01         &lt;none&gt;           &lt;none&gt;
root@controlplane:~# 
</pre></div>
</div>
</div>
<p>k8sversions</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">k8sversions</span><a class="headerlink" href="#id3" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>MacBook-Pro:Four_September_2021_CKA_Recap bharathdasaraju$ kubectl get nodes 
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   17d   v1.20.2
MacBook-Pro:Four_September_2021_CKA_Recap bharathdasaraju

v1.20.2  --&gt; major<span class="o">(</span>v1<span class="o">)</span>, minor<span class="o">(</span><span class="m">20</span><span class="o">)</span>, patch<span class="o">(</span><span class="m">2</span><span class="o">)</span>

besides that there is beta and alpha versions
</pre></div>
</div>
</div>
<p>ClusterUpgrade</p>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">ClusterUpgrade</span><a class="headerlink" href="#id4" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">1</span>. <span class="k">in</span> cloud providers k8s cluster can upgrade automatically

<span class="m">2</span>. <span class="k">if</span> k8s cluster deployed with kubeadm <span class="k">then</span>  <span class="m">1</span>. kubeadm upgrade plan  <span class="m">2</span>. kubeadm upgrade apply

<span class="m">3</span>. In hardway upgrade each component manually separately<span class="o">(</span><span class="m">1</span>.kube api-server <span class="m">2</span>.kube controller-manager <span class="m">3</span>. kube etcd server <span class="m">4</span>. Kube-scheduler <span class="m">5</span>.kubelet <span class="m">6</span>.kube-proxy<span class="o">)</span>
</pre></div>
</div>
</div>
<p>ClusterUpgradelabs</p>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">ClusterUpgradelabs</span><a class="headerlink" href="#id5" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

kubectl drain node01 --ignore-daemonsets <span class="o">==</span>&gt; This <span class="nb">command</span> should run on controlplane node<span class="o">(</span>Not <span class="k">in</span> the worker node<span class="o">)</span>


root@controlplane:~# kubectl get nodes
NAME           STATUS   ROLES    AGE   VERSION
controlplane   Ready    master   26m   v1.19.0
node01         Ready    &lt;none&gt;   25m   v1.19.0
root@controlplane:~# 

root@controlplane:~# cat /etc/*release*
<span class="nv">DISTRIB_ID</span><span class="o">=</span>Ubuntu
<span class="nv">DISTRIB_RELEASE</span><span class="o">=</span><span class="m">18</span>.04
<span class="nv">DISTRIB_CODENAME</span><span class="o">=</span>bionic
<span class="nv">DISTRIB_DESCRIPTION</span><span class="o">=</span><span class="s2">&quot;Ubuntu 18.04.5 LTS&quot;</span>
<span class="nv">NAME</span><span class="o">=</span><span class="s2">&quot;Ubuntu&quot;</span>
<span class="nv">VERSION</span><span class="o">=</span><span class="s2">&quot;18.04.5 LTS (Bionic Beaver)&quot;</span>
<span class="nv">ID</span><span class="o">=</span>ubuntu
<span class="nv">ID_LIKE</span><span class="o">=</span>debian
<span class="nv">PRETTY_NAME</span><span class="o">=</span><span class="s2">&quot;Ubuntu 18.04.5 LTS&quot;</span>
<span class="nv">VERSION_ID</span><span class="o">=</span><span class="s2">&quot;18.04&quot;</span>
<span class="nv">HOME_URL</span><span class="o">=</span><span class="s2">&quot;https://www.ubuntu.com/&quot;</span>
<span class="nv">SUPPORT_URL</span><span class="o">=</span><span class="s2">&quot;https://help.ubuntu.com/&quot;</span>
<span class="nv">BUG_REPORT_URL</span><span class="o">=</span><span class="s2">&quot;https://bugs.launchpad.net/ubuntu/&quot;</span>
<span class="nv">PRIVACY_POLICY_URL</span><span class="o">=</span><span class="s2">&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;</span>
<span class="nv">VERSION_CODENAME</span><span class="o">=</span>bionic
<span class="nv">UBUNTU_CODENAME</span><span class="o">=</span>bionic
root@controlplane:~# 



root@controlplane:~# kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-746c87566d-8tbgw   <span class="m">1</span>/1     Running   <span class="m">0</span>          83s   <span class="m">10</span>.244.0.5   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-gjtzg   <span class="m">1</span>/1     Running   <span class="m">0</span>          84s   <span class="m">10</span>.244.0.4   controlplane   &lt;none&gt;           &lt;none&gt;
blue-746c87566d-kn5pv   <span class="m">1</span>/1     Running   <span class="m">0</span>          84s   <span class="m">10</span>.244.1.3   node01         &lt;none&gt;           &lt;none&gt;
blue-746c87566d-rnbvw   <span class="m">1</span>/1     Running   <span class="m">0</span>          84s   <span class="m">10</span>.244.1.4   node01         &lt;none&gt;           &lt;none&gt;
blue-746c87566d-tp4m7   <span class="m">1</span>/1     Running   <span class="m">0</span>          83s   <span class="m">10</span>.244.1.5   node01         &lt;none&gt;           &lt;none&gt;
simple-webapp-1         <span class="m">1</span>/1     Running   <span class="m">0</span>          84s   <span class="m">10</span>.244.1.2   node01         &lt;none&gt;           &lt;none&gt;
root@controlplane:~# kubectl describe node node01 <span class="p">|</span> grep -i taint
Taints:             &lt;none&gt;
root@controlplane:~# kubectl describe node controlplane <span class="p">|</span> grep -i taint
Taints:             &lt;none&gt;
root@controlplane:~# 



root@controlplane:~# kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
blue-746c87566d-8tbgw   <span class="m">1</span>/1     Running   <span class="m">0</span>          2m25s
blue-746c87566d-gjtzg   <span class="m">1</span>/1     Running   <span class="m">0</span>          2m26s
blue-746c87566d-kn5pv   <span class="m">1</span>/1     Running   <span class="m">0</span>          2m26s
blue-746c87566d-rnbvw   <span class="m">1</span>/1     Running   <span class="m">0</span>          2m26s
blue-746c87566d-tp4m7   <span class="m">1</span>/1     Running   <span class="m">0</span>          2m25s
simple-webapp-1         <span class="m">1</span>/1     Running   <span class="m">0</span>          2m26s
root@controlplane:~# kubectl get deploy
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   <span class="m">5</span>/5     <span class="m">5</span>            <span class="m">5</span>           2m35s
root@controlplane:~# 




root@controlplane:~# kubeadm upgrade plan
<span class="o">[</span>upgrade/config<span class="o">]</span> Making sure the configuration is correct:
<span class="o">[</span>upgrade/config<span class="o">]</span> Reading configuration from the cluster...
<span class="o">[</span>upgrade/config<span class="o">]</span> FYI: You can look at this config file with <span class="s1">&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks.
<span class="o">[</span>upgrade<span class="o">]</span> Running cluster health checks
<span class="o">[</span>upgrade<span class="o">]</span> Fetching available versions to upgrade to
<span class="o">[</span>upgrade/versions<span class="o">]</span> Cluster version: v1.19.0
<span class="o">[</span>upgrade/versions<span class="o">]</span> kubeadm version: v1.19.0
I0921 <span class="m">13</span>:54:49.753378   <span class="m">24236</span> version.go:252<span class="o">]</span> remote version is much newer: v1.22.2<span class="p">;</span> falling back to: stable-1.19
<span class="o">[</span>upgrade/versions<span class="o">]</span> Latest stable version: v1.19.15
<span class="o">[</span>upgrade/versions<span class="o">]</span> Latest stable version: v1.19.15
<span class="o">[</span>upgrade/versions<span class="o">]</span> Latest version <span class="k">in</span> the v1.19 series: v1.19.15
<span class="o">[</span>upgrade/versions<span class="o">]</span> Latest version <span class="k">in</span> the v1.19 series: v1.19.15

Components that must be upgraded manually after you have upgraded the control plane with <span class="s1">&#39;kubeadm upgrade apply&#39;</span>:
COMPONENT   CURRENT       AVAILABLE
kubelet     <span class="m">2</span> x v1.19.0   v1.19.15

Upgrade to the latest version <span class="k">in</span> the v1.19 series:

COMPONENT                 CURRENT   AVAILABLE
kube-apiserver            v1.19.0   v1.19.15
kube-controller-manager   v1.19.0   v1.19.15
kube-scheduler            v1.19.0   v1.19.15
kube-proxy                v1.19.0   v1.19.15
CoreDNS                   <span class="m">1</span>.7.0     <span class="m">1</span>.7.0
etcd                      <span class="m">3</span>.4.9-1   <span class="m">3</span>.4.9-1

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.19.15

Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.15.

_____________________________________________________________________


The table below shows the current state of component configs as understood by this version of kubeadm.
Configs that have a <span class="s2">&quot;yes&quot;</span> mark <span class="k">in</span> the <span class="s2">&quot;MANUAL UPGRADE REQUIRED&quot;</span> column require manual config upgrade or
resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
upgrade to is denoted <span class="k">in</span> the <span class="s2">&quot;PREFERRED VERSION&quot;</span> column.

API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
kubelet.config.k8s.io     v1beta1           v1beta1             no
_____________________________________________________________________

root@controlplane:~# 



root@controlplane:~# kubectl drain controlplane --ignore-daemonsets
node/controlplane cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-jsvcl, kube-system/kube-proxy-mqz2l
evicting pod default/blue-746c87566d-8tbgw
evicting pod default/blue-746c87566d-gjtzg
evicting pod kube-system/coredns-f9fd979d6-5sgvw
evicting pod kube-system/coredns-f9fd979d6-6k5kr
pod/blue-746c87566d-gjtzg evicted
pod/coredns-f9fd979d6-5sgvw evicted
pod/blue-746c87566d-8tbgw evicted
pod/coredns-f9fd979d6-6k5kr evicted
node/controlplane evicted




Upgrade the controlplane components to exact version v1.20.0
                      Controlplane_node:
----------------------------------------------------------------&gt;


Upgrade kubeadm tool <span class="o">(</span><span class="k">if</span> not already<span class="o">)</span>, <span class="k">then</span> the master components, and finally the kubelet. 
Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, 
<span class="k">if</span> you hit dependency issue <span class="k">while</span> running the apt-get upgrade kubelet command, use the apt install <span class="nv">kubelet</span><span class="o">=</span><span class="m">1</span>.20.0-00 <span class="nb">command</span> instead



root@controlplane:~# apt update
Hit:2 https://download.docker.com/linux/ubuntu bionic InRelease                                                 
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease <span class="o">[</span><span class="m">9383</span> B<span class="o">]</span>                                
Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease                                               
Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease                                                         
Hit:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease
Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease
Fetched <span class="m">9383</span> B <span class="k">in</span> 3s <span class="o">(</span><span class="m">3194</span> B/s<span class="o">)</span>
Reading package lists... Done
Building dependency tree       
Reading state information... Done
<span class="m">29</span> packages can be upgraded. Run <span class="s1">&#39;apt list --upgradable&#39;</span> to see them.
root@controlplane:~# 


root@controlplane:~# apt install <span class="nv">kubeadm</span><span class="o">=</span><span class="m">1</span>.20.0-00 -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following held packages will be changed:
  kubeadm
The following packages will be upgraded:
  kubeadm
<span class="m">1</span> upgraded, <span class="m">0</span> newly installed, <span class="m">0</span> to remove and <span class="m">28</span> not upgraded.
Need to get <span class="m">7707</span> kB of archives.
After this operation, <span class="m">111</span> kB of additional disk space will be used.
Do you want to <span class="k">continue</span>? <span class="o">[</span>Y/n<span class="o">]</span> Y
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 <span class="m">1</span>.20.0-00 <span class="o">[</span><span class="m">7707</span> kB<span class="o">]</span>
Fetched <span class="m">7707</span> kB <span class="k">in</span> 0s <span class="o">(</span><span class="m">16</span>.8 MB/s<span class="o">)</span>
debconf: delaying package configuration, since apt-utils is not installed
<span class="o">(</span>Reading database ... <span class="m">15149</span> files and directories currently installed.<span class="o">)</span>
Preparing to unpack .../kubeadm_1.20.0-00_amd64.deb ...
Unpacking kubeadm <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> over <span class="o">(</span><span class="m">1</span>.19.0-00<span class="o">)</span> ...
Setting up kubeadm <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> ...
root@controlplane:~# 



root@controlplane:~# kubeadm upgrade apply v1.20.0
<span class="o">[</span>upgrade/config<span class="o">]</span> Making sure the configuration is correct:
<span class="o">[</span>upgrade/config<span class="o">]</span> Reading configuration from the cluster...
<span class="o">[</span>upgrade/config<span class="o">]</span> FYI: You can look at this config file with <span class="s1">&#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;</span>
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks.
<span class="o">[</span>upgrade<span class="o">]</span> Running cluster health checks
<span class="o">[</span>upgrade/version<span class="o">]</span> You have chosen to change the cluster version to <span class="s2">&quot;v1.20.0&quot;</span>
<span class="o">[</span>upgrade/versions<span class="o">]</span> Cluster version: v1.19.0
<span class="o">[</span>upgrade/versions<span class="o">]</span> kubeadm version: v1.20.0
<span class="o">[</span>upgrade/confirm<span class="o">]</span> Are you sure you want to proceed with the upgrade? <span class="o">[</span>y/N<span class="o">]</span>: y
<span class="o">[</span>upgrade/prepull<span class="o">]</span> Pulling images required <span class="k">for</span> setting up a Kubernetes cluster
<span class="o">[</span>upgrade/prepull<span class="o">]</span> This might take a minute or two, depending on the speed of your internet connection
<span class="o">[</span>upgrade/prepull<span class="o">]</span> You can also perform this action <span class="k">in</span> beforehand using <span class="s1">&#39;kubeadm config images pull&#39;</span>
<span class="o">[</span>upgrade/apply<span class="o">]</span> Upgrading your Static Pod-hosted control plane to version <span class="s2">&quot;v1.20.0&quot;</span>...
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-controller-manager-controlplane hash: f6a9bf2865b2fe580f39f07ed872106b
Static pod: kube-scheduler-controlplane hash: 5146743ebb284c11f03dc85146799d8b
<span class="o">[</span>upgrade/etcd<span class="o">]</span> Upgrading to TLS <span class="k">for</span> etcd
Static pod: etcd-controlplane hash: 9329872a3917b49cc78c2b1ed2e6439f
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Preparing <span class="k">for</span> <span class="s2">&quot;etcd&quot;</span> upgrade
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing etcd-server certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing etcd-peer certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing etcd-healthcheck-client certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Moved new manifest to <span class="s2">&quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span> and backed up old manifest to <span class="s2">&quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-09-21-14-05-49/etcd.yaml&quot;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to restart the component
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> This might take a minute or longer depending on the component/version gap <span class="o">(</span>timeout 5m0s<span class="o">)</span>
Static pod: etcd-controlplane hash: 9329872a3917b49cc78c2b1ed2e6439f
Static pod: etcd-controlplane hash: 3cde3d9caab5089311d6d64f30417d52
<span class="o">[</span>apiclient<span class="o">]</span> Found <span class="m">1</span> Pods <span class="k">for</span> label selector <span class="nv">component</span><span class="o">=</span>etcd
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Component <span class="s2">&quot;etcd&quot;</span> upgraded successfully!
<span class="o">[</span>upgrade/etcd<span class="o">]</span> Waiting <span class="k">for</span> etcd to become available
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Writing new Static Pod manifests to <span class="s2">&quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests363947080&quot;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Preparing <span class="k">for</span> <span class="s2">&quot;kube-apiserver&quot;</span> upgrade
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing apiserver certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing apiserver-kubelet-client certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing front-proxy-client certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing apiserver-etcd-client certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Moved new manifest to <span class="s2">&quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span> and backed up old manifest to <span class="s2">&quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-09-21-14-05-49/kube-apiserver.yaml&quot;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to restart the component
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> This might take a minute or longer depending on the component/version gap <span class="o">(</span>timeout 5m0s<span class="o">)</span>
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: c55bca267c1fd7f395e6c47f180272bb
Static pod: kube-apiserver-controlplane hash: cb4f658ebd1182de2f60cf8a9d8181c5
<span class="o">[</span>apiclient<span class="o">]</span> Found <span class="m">1</span> Pods <span class="k">for</span> label selector <span class="nv">component</span><span class="o">=</span>kube-apiserver
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Component <span class="s2">&quot;kube-apiserver&quot;</span> upgraded successfully!
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Preparing <span class="k">for</span> <span class="s2">&quot;kube-controller-manager&quot;</span> upgrade
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing controller-manager.conf certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Moved new manifest to <span class="s2">&quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span> and backed up old manifest to <span class="s2">&quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-09-21-14-05-49/kube-controller-manager.yaml&quot;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to restart the component
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> This might take a minute or longer depending on the component/version gap <span class="o">(</span>timeout 5m0s<span class="o">)</span>
Static pod: kube-controller-manager-controlplane hash: f6a9bf2865b2fe580f39f07ed872106b
Static pod: kube-controller-manager-controlplane hash: a875134e700993a22f67999011829566
<span class="o">[</span>apiclient<span class="o">]</span> Found <span class="m">1</span> Pods <span class="k">for</span> label selector <span class="nv">component</span><span class="o">=</span>kube-controller-manager
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Component <span class="s2">&quot;kube-controller-manager&quot;</span> upgraded successfully!
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Preparing <span class="k">for</span> <span class="s2">&quot;kube-scheduler&quot;</span> upgrade
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Renewing scheduler.conf certificate
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Moved new manifest to <span class="s2">&quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span> and backed up old manifest to <span class="s2">&quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-09-21-14-05-49/kube-scheduler.yaml&quot;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to restart the component
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> This might take a minute or longer depending on the component/version gap <span class="o">(</span>timeout 5m0s<span class="o">)</span>
Static pod: kube-scheduler-controlplane hash: 5146743ebb284c11f03dc85146799d8b
Static pod: kube-scheduler-controlplane hash: 81d2d21449d64d5e6d5e9069a7ca99ed
<span class="o">[</span>apiclient<span class="o">]</span> Found <span class="m">1</span> Pods <span class="k">for</span> label selector <span class="nv">component</span><span class="o">=</span>kube-scheduler
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Component <span class="s2">&quot;kube-scheduler&quot;</span> upgraded successfully!
<span class="o">[</span>upgrade/postupgrade<span class="o">]</span> Applying label node-role.kubernetes.io/control-plane<span class="o">=</span><span class="s1">&#39;&#39;</span> to Nodes with label node-role.kubernetes.io/master<span class="o">=</span><span class="s1">&#39;&#39;</span> <span class="o">(</span>deprecated<span class="o">)</span>
<span class="o">[</span>upload-config<span class="o">]</span> Storing the configuration used <span class="k">in</span> ConfigMap <span class="s2">&quot;kubeadm-config&quot;</span> <span class="k">in</span> the <span class="s2">&quot;kube-system&quot;</span> Namespace
<span class="o">[</span>kubelet<span class="o">]</span> Creating a ConfigMap <span class="s2">&quot;kubelet-config-1.20&quot;</span> <span class="k">in</span> namespace kube-system with the configuration <span class="k">for</span> the kubelets <span class="k">in</span> the cluster
<span class="o">[</span>kubelet-start<span class="o">]</span> Writing kubelet configuration to file <span class="s2">&quot;/var/lib/kubelet/config.yaml&quot;</span>
<span class="o">[</span>bootstrap-token<span class="o">]</span> configured RBAC rules to allow Node Bootstrap tokens to get nodes
<span class="o">[</span>bootstrap-token<span class="o">]</span> configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="k">in</span> order <span class="k">for</span> nodes to get long term certificate credentials
<span class="o">[</span>bootstrap-token<span class="o">]</span> configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
<span class="o">[</span>bootstrap-token<span class="o">]</span> configured RBAC rules to allow certificate rotation <span class="k">for</span> all node client certificates <span class="k">in</span> the cluster
<span class="o">[</span>addons<span class="o">]</span> Applied essential addon: CoreDNS
<span class="o">[</span>addons<span class="o">]</span> Applied essential addon: kube-proxy

<span class="o">[</span>upgrade/successful<span class="o">]</span> SUCCESS! Your cluster was upgraded to <span class="s2">&quot;v1.20.0&quot;</span>. Enjoy!

<span class="o">[</span>upgrade/kubelet<span class="o">]</span> Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="k">if</span> you havent already <span class="k">done</span> so.
root@controlplane:~# 


root@controlplane:~# apt install <span class="nv">kubelet</span><span class="o">=</span><span class="m">1</span>.20.0-00
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following held packages will be changed:
  kubelet
The following packages will be upgraded:
  kubelet
<span class="m">1</span> upgraded, <span class="m">0</span> newly installed, <span class="m">0</span> to remove and <span class="m">28</span> not upgraded.
Need to get <span class="m">18</span>.8 MB of archives.
After this operation, <span class="m">4000</span> kB of additional disk space will be used.
Do you want to <span class="k">continue</span>? <span class="o">[</span>Y/n<span class="o">]</span> Y
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 <span class="m">1</span>.20.0-00 <span class="o">[</span><span class="m">18</span>.8 MB<span class="o">]</span>
Fetched <span class="m">18</span>.8 MB <span class="k">in</span> 1s <span class="o">(</span><span class="m">20</span>.4 MB/s<span class="o">)</span>  
debconf: delaying package configuration, since apt-utils is not installed
<span class="o">(</span>Reading database ... <span class="m">15149</span> files and directories currently installed.<span class="o">)</span>
Preparing to unpack .../kubelet_1.20.0-00_amd64.deb ...
/usr/sbin/policy-rc.d returned <span class="m">101</span>, not running <span class="s1">&#39;stop kubelet.service&#39;</span>
Unpacking kubelet <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> over <span class="o">(</span><span class="m">1</span>.19.0-00<span class="o">)</span> ...
Setting up kubelet <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> ...
/usr/sbin/policy-rc.d returned <span class="m">101</span>, not running <span class="s1">&#39;start kubelet.service&#39;</span>
root@controlplane:~#


root@controlplane:~# systemctl restart kubelet
root@controlplane:~# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded <span class="o">(</span>/lib/systemd/system/kubelet.service<span class="p">;</span> enabled<span class="p">;</span> vendor preset: enabled<span class="o">)</span>
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active <span class="o">(</span>running<span class="o">)</span> since Tue <span class="m">2021</span>-09-21 <span class="m">14</span>:14:03 UTC<span class="p">;</span> 12s ago
     Docs: https://kubernetes.io/docs/home/
 Main PID: <span class="m">2585</span> <span class="o">(</span>kubelet<span class="o">)</span>
    Tasks: <span class="m">30</span> <span class="o">(</span>limit: <span class="m">5529</span><span class="o">)</span>
   CGroup: /system.slice/kubelet.service
           └─2585 /usr/bin/kubelet --bootstrap-kubeconfig<span class="o">=</span>/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig<span class="o">=</span>/et

Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414706    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414739    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414766    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414829    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414879    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414901    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414923    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414949    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.414977    <span class="m">2585</span> reconciler.go:224<span class="o">]</span> operationExecutor.Ve
Sep <span class="m">21</span> <span class="m">14</span>:14:08 controlplane kubelet<span class="o">[</span><span class="m">2585</span><span class="o">]</span>: I0921 <span class="m">14</span>:14:08.415051    <span class="m">2585</span> reconciler.go:157<span class="o">]</span> Reconciler: start to
root@controlplane:~# 


root@controlplane:~# kubectl uncordon controlplane
node/controlplane uncordoned
root@controlplane:~# 


Drain the worker node of the workloads and mark it UnSchedulable
                      Worker_node:
----------------------------------------------------------------&gt;

root@controlplane:~# kubectl drain node01 --ignore-daemonsets --force
node/node01 already cordoned
WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/simple-webapp-1<span class="p">;</span> ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-k6dmc, kube-system/kube-proxy-7jdgp
evicting pod default/blue-746c87566d-kn5pv
evicting pod default/blue-746c87566d-c2ct8
evicting pod default/blue-746c87566d-rnbvw
evicting pod default/simple-webapp-1
evicting pod default/blue-746c87566d-xzkw5
evicting pod kube-system/coredns-74ff55c5b-mfqvt
evicting pod kube-system/coredns-74ff55c5b-w4n4s
evicting pod default/blue-746c87566d-tp4m7
I0921 <span class="m">14</span>:17:46.427327    <span class="m">4653</span> request.go:645<span class="o">]</span> Throttling request took <span class="m">1</span>.009198612s, request: GET:https://controlplane:6443/api/v1/namespaces/default/pods/blue-746c87566d-xzkw5
pod/blue-746c87566d-tp4m7 evicted
pod/blue-746c87566d-kn5pv evicted
pod/blue-746c87566d-rnbvw evicted
pod/coredns-74ff55c5b-mfqvt evicted
pod/blue-746c87566d-c2ct8 evicted
pod/coredns-74ff55c5b-w4n4s evicted
pod/blue-746c87566d-xzkw5 evicted
pod/simple-webapp-1 evicted
node/node01 evicted
root@controlplane:~#



root@node01:~# apt update -y
Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease <span class="o">[</span><span class="m">88</span>.7 kB<span class="o">]</span>                                     
Get:3 https://download.docker.com/linux/ubuntu bionic InRelease <span class="o">[</span><span class="m">64</span>.4 kB<span class="o">]</span>                                       
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease <span class="o">[</span><span class="m">9383</span> B<span class="o">]</span>                                
Get:4 http://archive.ubuntu.com/ubuntu bionic InRelease <span class="o">[</span><span class="m">242</span> kB<span class="o">]</span>                   
Get:5 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages <span class="o">[</span><span class="m">26</span>.7 kB<span class="o">]</span>
Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages <span class="o">[</span><span class="m">1428</span> kB<span class="o">]</span>                  
Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages <span class="o">[</span><span class="m">22</span>.4 kB<span class="o">]</span>        
Get:8 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages <span class="o">[</span><span class="m">567</span> kB<span class="o">]</span>                      
Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease <span class="o">[</span><span class="m">88</span>.7 kB<span class="o">]</span>                               
Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages <span class="o">[</span><span class="m">2326</span> kB<span class="o">]</span>                          
Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease <span class="o">[</span><span class="m">74</span>.6 kB<span class="o">]</span>                              
Get:12 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages <span class="o">[</span><span class="m">50</span>.0 kB<span class="o">]</span>                    
Get:13 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages <span class="o">[</span><span class="m">13</span>.5 kB<span class="o">]</span> 
Get:14 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages <span class="o">[</span><span class="m">186</span> kB<span class="o">]</span>     
Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages <span class="o">[</span><span class="m">11</span>.3 MB<span class="o">]</span>
Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages <span class="o">[</span><span class="m">1344</span> kB<span class="o">]</span>          
Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages <span class="o">[</span><span class="m">600</span> kB<span class="o">]</span>
Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages <span class="o">[</span><span class="m">2202</span> kB<span class="o">]</span>
Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages <span class="o">[</span><span class="m">34</span>.4 kB<span class="o">]</span>
Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages <span class="o">[</span><span class="m">2761</span> kB<span class="o">]</span>
Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages <span class="o">[</span><span class="m">11</span>.4 kB<span class="o">]</span>
Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages <span class="o">[</span><span class="m">11</span>.3 kB<span class="o">]</span>
Fetched <span class="m">23</span>.5 MB <span class="k">in</span> 6s <span class="o">(</span><span class="m">3830</span> kB/s<span class="o">)</span>                                                                               
Reading package lists... Done
Building dependency tree       
Reading state information... Done
<span class="m">35</span> packages can be upgraded. Run <span class="s1">&#39;apt list --upgradable&#39;</span> to see them.
root@node01:~# 


root@node01:~# apt install <span class="nv">kubeadm</span><span class="o">=</span><span class="m">1</span>.20.0-00
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following held packages will be changed:
  kubeadm
The following packages will be upgraded:
  kubeadm
<span class="m">1</span> upgraded, <span class="m">0</span> newly installed, <span class="m">0</span> to remove and <span class="m">34</span> not upgraded.
Need to get <span class="m">7707</span> kB of archives.
After this operation, <span class="m">111</span> kB of additional disk space will be used.
Do you want to <span class="k">continue</span>? <span class="o">[</span>Y/n<span class="o">]</span> Y
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 <span class="m">1</span>.20.0-00 <span class="o">[</span><span class="m">7707</span> kB<span class="o">]</span>
Fetched <span class="m">7707</span> kB <span class="k">in</span> 0s <span class="o">(</span><span class="m">34</span>.3 MB/s<span class="o">)</span>
debconf: delaying package configuration, since apt-utils is not installed
<span class="o">(</span>Reading database ... <span class="m">15013</span> files and directories currently installed.<span class="o">)</span>
Preparing to unpack .../kubeadm_1.20.0-00_amd64.deb ...
Unpacking kubeadm <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> over <span class="o">(</span><span class="m">1</span>.19.0-00<span class="o">)</span> ...
Setting up kubeadm <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> ...
root@node01:~# 

root@node01:~# kubeadm upgrade node
<span class="o">[</span>upgrade<span class="o">]</span> Reading configuration from the cluster...
<span class="o">[</span>upgrade<span class="o">]</span> FYI: You can look at this config file with <span class="s1">&#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;</span>
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks
<span class="o">[</span>preflight<span class="o">]</span> Skipping prepull. Not a control plane node.
<span class="o">[</span>upgrade<span class="o">]</span> Skipping phase. Not a control plane node.
<span class="o">[</span>kubelet-start<span class="o">]</span> Writing kubelet configuration to file <span class="s2">&quot;/var/lib/kubelet/config.yaml&quot;</span>
<span class="o">[</span>upgrade<span class="o">]</span> The configuration <span class="k">for</span> this node was successfully updated!
<span class="o">[</span>upgrade<span class="o">]</span> Now you should go ahead and upgrade the kubelet package using your package manager.
root@node01:~#



root@node01:~# apt install <span class="nv">kubelet</span><span class="o">=</span><span class="m">1</span>.20.0-00
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following held packages will be changed:
  kubelet
The following packages will be upgraded:
  kubelet
<span class="m">1</span> upgraded, <span class="m">0</span> newly installed, <span class="m">0</span> to remove and <span class="m">34</span> not upgraded.
Need to get <span class="m">18</span>.8 MB of archives.
After this operation, <span class="m">4000</span> kB of additional disk space will be used.
Do you want to <span class="k">continue</span>? <span class="o">[</span>Y/n<span class="o">]</span> Y
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 <span class="m">1</span>.20.0-00 <span class="o">[</span><span class="m">18</span>.8 MB<span class="o">]</span>
Fetched <span class="m">18</span>.8 MB <span class="k">in</span> 1s <span class="o">(</span><span class="m">31</span>.1 MB/s<span class="o">)</span>
debconf: delaying package configuration, since apt-utils is not installed
<span class="o">(</span>Reading database ... <span class="m">15013</span> files and directories currently installed.<span class="o">)</span>
Preparing to unpack .../kubelet_1.20.0-00_amd64.deb ...
/usr/sbin/policy-rc.d returned <span class="m">101</span>, not running <span class="s1">&#39;stop kubelet.service&#39;</span>
Unpacking kubelet <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> over <span class="o">(</span><span class="m">1</span>.19.0-00<span class="o">)</span> ...
Setting up kubelet <span class="o">(</span><span class="m">1</span>.20.0-00<span class="o">)</span> ...
/usr/sbin/policy-rc.d returned <span class="m">101</span>, not running <span class="s1">&#39;start kubelet.service&#39;</span>
root@node01:~#


root@node01:~# systemctl restart kubelet
root@node01:~# systemctl status  kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded <span class="o">(</span>/lib/systemd/system/kubelet.service<span class="p">;</span> enabled<span class="p">;</span> vendor preset: enabled<span class="o">)</span>
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active <span class="o">(</span>running<span class="o">)</span> since Tue <span class="m">2021</span>-09-21 <span class="m">14</span>:28:24 UTC<span class="p">;</span> 13s ago
     Docs: https://kubernetes.io/docs/home/
 Main PID: <span class="m">46591</span> <span class="o">(</span>kubelet<span class="o">)</span>
    Tasks: <span class="m">32</span> <span class="o">(</span>limit: <span class="m">7372</span><span class="o">)</span>
   CGroup: /system.slice/kubelet.service
           └─46591 /usr/bin/kubelet --bootstrap-kubeconfig<span class="o">=</span>/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig<span class="o">=</span>/e

Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.514366   <span class="m">46591</span> topology_manager.go:187<span class="o">]</span> <span class="o">[</span>topologymanager<span class="o">]</span> T
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.521710   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.521780   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.521806   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.521830   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.521910   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.521953   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.521981   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.522015   <span class="m">46591</span> reconciler.go:224<span class="o">]</span> operationExecutor.VerifyC
Sep <span class="m">21</span> <span class="m">14</span>:28:26 node01 kubelet<span class="o">[</span><span class="m">46591</span><span class="o">]</span>: I0921 <span class="m">14</span>:28:26.522052   <span class="m">46591</span> reconciler.go:157<span class="o">]</span> Reconciler: start to sync
root@node01:~# 



root@controlplane:~# kubectl get nodes
NAME           STATUS   ROLES                  AGE   VERSION
controlplane   Ready    control-plane,master   69m   v1.20.0
node01         Ready    &lt;none&gt;                 69m   v1.20.0
root@controlplane:~# 
</pre></div>
</div>
</div>
<p>etcdbackup</p>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">etcdbackup</span><a class="headerlink" href="#id6" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>what to backup <span class="k">in</span> k8s cluster

<span class="m">1</span>. Resource configuration.
<span class="m">2</span>. Etcd cluster.
<span class="m">3</span>. Persistent storage of the application.


Get all resource configuration:
-----------------------------------&gt;
kubectl get all --all-namespaces -o yaml &gt; all-deploy-services.yaml
there are tools <span class="k">for</span> this like velero to backup



Backup etcd cluster:
-------------------------&gt;
etcd.service:  --data-dir<span class="o">=</span>/var/lib/etcd  we can backup this directory

etcd by default comes with <span class="s2">&quot;snapshot&quot;</span> option.
<span class="m">1</span>. <span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> etcdctl snapshot save snapshot.db


MacBook-Pro:Four_September_2021_CKA_Recap bharathdasaraju$ kubectl <span class="nb">exec</span> etcd-minikube -n kube-system -- etcdctl --endpoints https://192.168.49.2:2379 --cacert /var/lib/minikube/certs/etcd/ca.crt --cert /var/lib/minikube/certs/etcd/server.crt --key /var/lib/minikube/certs/etcd/server.key snapshot save snapshot.db
<span class="o">{</span><span class="s2">&quot;level&quot;</span>:<span class="s2">&quot;info&quot;</span>,<span class="s2">&quot;ts&quot;</span>:1632264347.5080614,<span class="s2">&quot;caller&quot;</span>:<span class="s2">&quot;snapshot/v3_snapshot.go:119&quot;</span>,<span class="s2">&quot;msg&quot;</span>:<span class="s2">&quot;created temporary db file&quot;</span>,<span class="s2">&quot;path&quot;</span>:<span class="s2">&quot;snapshot.db.part&quot;</span><span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;level&quot;</span>:<span class="s2">&quot;info&quot;</span>,<span class="s2">&quot;ts&quot;</span>:<span class="s2">&quot;2021-09-21T22:45:47.532Z&quot;</span>,<span class="s2">&quot;caller&quot;</span>:<span class="s2">&quot;clientv3/maintenance.go:200&quot;</span>,<span class="s2">&quot;msg&quot;</span>:<span class="s2">&quot;opened snapshot stream; downloading&quot;</span><span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;level&quot;</span>:<span class="s2">&quot;info&quot;</span>,<span class="s2">&quot;ts&quot;</span>:1632264347.5378127,<span class="s2">&quot;caller&quot;</span>:<span class="s2">&quot;snapshot/v3_snapshot.go:127&quot;</span>,<span class="s2">&quot;msg&quot;</span>:<span class="s2">&quot;fetching snapshot&quot;</span>,<span class="s2">&quot;endpoint&quot;</span>:<span class="s2">&quot;https://192.168.49.2:2379&quot;</span><span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;level&quot;</span>:<span class="s2">&quot;info&quot;</span>,<span class="s2">&quot;ts&quot;</span>:<span class="s2">&quot;2021-09-21T22:45:47.669Z&quot;</span>,<span class="s2">&quot;caller&quot;</span>:<span class="s2">&quot;clientv3/maintenance.go:208&quot;</span>,<span class="s2">&quot;msg&quot;</span>:<span class="s2">&quot;completed snapshot read; closing&quot;</span><span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;level&quot;</span>:<span class="s2">&quot;info&quot;</span>,<span class="s2">&quot;ts&quot;</span>:1632264347.6795833,<span class="s2">&quot;caller&quot;</span>:<span class="s2">&quot;snapshot/v3_snapshot.go:142&quot;</span>,<span class="s2">&quot;msg&quot;</span>:<span class="s2">&quot;fetched snapshot&quot;</span>,<span class="s2">&quot;endpoint&quot;</span>:<span class="s2">&quot;https://192.168.49.2:2379&quot;</span>,<span class="s2">&quot;size&quot;</span>:<span class="s2">&quot;3.0 MB&quot;</span>,<span class="s2">&quot;took&quot;</span>:0.1696802<span class="o">}</span>
<span class="o">{</span><span class="s2">&quot;level&quot;</span>:<span class="s2">&quot;info&quot;</span>,<span class="s2">&quot;ts&quot;</span>:1632264347.6797824,<span class="s2">&quot;caller&quot;</span>:<span class="s2">&quot;snapshot/v3_snapshot.go:152&quot;</span>,<span class="s2">&quot;msg&quot;</span>:<span class="s2">&quot;saved&quot;</span>,<span class="s2">&quot;path&quot;</span>:<span class="s2">&quot;snapshot.db&quot;</span><span class="o">}</span>
Snapshot saved at snapshot.db
MacBook-Pro:Four_September_2021_CKA_Recap bharathdasaraju$

MacBook-Pro:Four_September_2021_CKA_Recap bharathdasaraju$ kubectl <span class="nb">exec</span> etcd-minikube -n kube-system -- etcdctl --endpoints https://192.168.49.2:2379 --cacert /var/lib/minikube/certs/etcd/ca.crt --cert /var/lib/minikube/certs/etcd/server.crt --key /var/lib/minikube/certs/etcd/server.key snapshot status snapshot.db
52a22ad4, <span class="m">119706</span>, <span class="m">821</span>, <span class="m">3</span>.0 MB
MacBook-Pro:Four_September_2021_CKA_Recap bharathdasaraju$ 


How to restore etcd from backup:
------------------------------------------------------&gt;
<span class="m">1</span>. stop kube-apiserver --&gt; service kube-apiserver stop
<span class="m">2</span>. Restore from backup --&gt; <span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup
<span class="m">3</span>. and configure this new data directory <span class="k">in</span> etcd.service as --data-dir<span class="o">=</span>/var/lib/etcd-from-backup
<span class="m">4</span>. systemctl daemon-reload
<span class="m">5</span>. service etcd restart 
<span class="m">6</span>. service kube-apiserver start 
</pre></div>
</div>
</div>
<p>etcdrestore</p>
<div class="literal-block-wrapper docutils container" id="id7">
<div class="code-block-caption"><span class="caption-text">etcdrestore</span><a class="headerlink" href="#id7" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>root@controlplane:~# kubectl get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/blue-746c87566d-6bdmd   <span class="m">1</span>/1     Running   <span class="m">0</span>          35s
pod/blue-746c87566d-757lx   <span class="m">1</span>/1     Running   <span class="m">0</span>          34s
pod/blue-746c87566d-rt6sn   <span class="m">1</span>/1     Running   <span class="m">0</span>          34s
pod/red-75f847bf79-ktzcc    <span class="m">1</span>/1     Running   <span class="m">0</span>          35s
pod/red-75f847bf79-phdd7    <span class="m">1</span>/1     Running   <span class="m">0</span>          35s

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>        AGE
service/blue-service   NodePort    <span class="m">10</span>.111.50.175   &lt;none&gt;        <span class="m">80</span>:30082/TCP   35s
service/kubernetes     ClusterIP   <span class="m">10</span>.96.0.1       &lt;none&gt;        <span class="m">443</span>/TCP        10m
service/red-service    NodePort    <span class="m">10</span>.100.207.17   &lt;none&gt;        <span class="m">80</span>:30080/TCP   35s

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/blue   <span class="m">3</span>/3     <span class="m">3</span>            <span class="m">3</span>           35s
deployment.apps/red    <span class="m">2</span>/2     <span class="m">2</span>            <span class="m">2</span>           35s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/blue-746c87566d   <span class="m">3</span>         <span class="m">3</span>         <span class="m">3</span>       35s
replicaset.apps/red-75f847bf79    <span class="m">2</span>         <span class="m">2</span>         <span class="m">2</span>       35s
root@controlplane:~# 


root@controlplane:~# ps -eaf <span class="p">|</span> grep -i <span class="s2">&quot;etcd &quot;</span>
root      <span class="m">3998</span>  <span class="m">3888</span>  <span class="m">0</span> <span class="m">22</span>:46 ?        <span class="m">00</span>:00:22 etcd --advertise-client-urls<span class="o">=</span>https://10.57.35.3:2379 --cert-file<span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt --client-cert-auth<span class="o">=</span><span class="nb">true</span> --data-dir<span class="o">=</span>/var/lib/etcd --initial-advertise-peer-urls<span class="o">=</span>https://10.57.35.3:2380 --initial-cluster<span class="o">=</span><span class="nv">controlplane</span><span class="o">=</span>https://10.57.35.3:2380 --key-file<span class="o">=</span>/etc/kubernetes/pki/etcd/server.key --listen-client-urls<span class="o">=</span>https://127.0.0.1:2379,https://10.57.35.3:2379 --listen-metrics-urls<span class="o">=</span>http://127.0.0.1:2381 --listen-peer-urls<span class="o">=</span>https://10.57.35.3:2380 --name<span class="o">=</span>controlplane --peer-cert-file<span class="o">=</span>/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth<span class="o">=</span><span class="nb">true</span> --peer-key-file<span class="o">=</span>/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file<span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt --snapshot-count<span class="o">=</span><span class="m">10000</span> --trusted-ca-file<span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt
root     <span class="m">15626</span> <span class="m">12165</span>  <span class="m">0</span> <span class="m">22</span>:57 pts/0    <span class="m">00</span>:00:00 grep --color<span class="o">=</span>auto -i etcd 
root@controlplane:~# 

root@controlplane:~# kubectl logs etcd-controlplane -n kube-system <span class="p">|</span> grep -i <span class="s2">&quot;version&quot;</span>
<span class="m">2021</span>-09-21 <span class="m">22</span>:46:41.918478 I <span class="p">|</span> etcdmain: etcd Version: <span class="m">3</span>.4.13
<span class="m">2021</span>-09-21 <span class="m">22</span>:46:41.918578 I <span class="p">|</span> etcdmain: Go Version: go1.12.17
<span class="m">2021</span>-09-21 <span class="m">22</span>:46:42.217126 I <span class="p">|</span> etcdserver: starting server... <span class="o">[</span>version: <span class="m">3</span>.4.13, cluster version: to_be_decided<span class="o">]</span>
<span class="m">2021</span>-09-21 <span class="m">22</span>:46:42.407653 I <span class="p">|</span> etcdserver: setting up the initial cluster version to <span class="m">3</span>.4
<span class="m">2021</span>-09-21 <span class="m">22</span>:46:42.407968 N <span class="p">|</span> etcdserver/membership: <span class="nb">set</span> the initial cluster version to <span class="m">3</span>.4
<span class="m">2021</span>-09-21 <span class="m">22</span>:46:42.408282 I <span class="p">|</span> etcdserver/api: enabled capabilities <span class="k">for</span> version <span class="m">3</span>.4
root@controlplane:~# kubectl describe pod etcd-controlplane -n kube-system <span class="p">|</span> grep -i version
root@controlplane:~# kubectl describe pod etcd-controlplane -n kube-system <span class="p">|</span> grep -i image  
    Image:         k8s.gcr.io/etcd:3.4.13-0
    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2
root@controlplane:~# 


etcd:
--cert<span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt
--cacert<span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt
--key<span class="o">=</span>/etc/kubernetes/pki/etcd/server.key

Backup etcd cluster:
------------------------&gt;
root@controlplane:/etc/kubernetes/pki/etcd# <span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> etcdctl --endpoints<span class="o">=</span>https://127.0.0.1:2379 <span class="se">\</span>
&gt; --cert<span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt --cacert<span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt <span class="se">\</span>
&gt; --key<span class="o">=</span>/etc/kubernetes/pki/etcd/server.key snapshot save /opt/snapshot-pre-boot.db
Snapshot saved at /opt/snapshot-pre-boot.db
root@controlplane:/etc/kubernetes/pki/etcd#


root@controlplane:/etc/kubernetes/pki/etcd# <span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> etcdctl --endpoints<span class="o">=</span>https://127.0.0.1:2379 --cert<span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt --cacert<span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt --key<span class="o">=</span>/etc/kubernetes/pki/etcd/server.key snapshot status /opt/snapshot-pre-boot.db
<span class="m">40896334</span>, <span class="m">2161</span>, <span class="m">916</span>, <span class="m">2</span>.2 MB
root@controlplane:/etc/kubernetes/pki/etcd# 



restore etcd cluster:
------------------------&gt;

root@controlplane:~# <span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> etcdctl snapshot restore /opt/snapshot-pre-boot.db --data-dir /var/lib/etcd-from-backup
<span class="m">2021</span>-09-21 <span class="m">23</span>:15:35.337656 I <span class="p">|</span> mvcc: restore compact to <span class="m">1646</span>
<span class="m">2021</span>-09-21 <span class="m">23</span>:15:35.350165 I <span class="p">|</span> etcdserver/membership: added member 8e9e05c52164694d <span class="o">[</span>http://localhost:2380<span class="o">]</span> to cluster cdf818194e3a8c32
root@controlplane:~# 


Next, update the /etc/kubernetes/manifests/etcd.yaml to use new --data-dir as  /var/lib/etcd-from-backup



<span class="o">=============================================================================================================================================================</span>&gt;

First Restore the snapshot:

root@controlplane:~# <span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> etcdctl  --data-dir /var/lib/etcd-from-backup <span class="se">\</span>
snapshot restore /opt/snapshot-pre-boot.db


<span class="m">2021</span>-03-25 <span class="m">23</span>:52:59.608547 I <span class="p">|</span> mvcc: restore compact to <span class="m">6466</span>
<span class="m">2021</span>-03-25 <span class="m">23</span>:52:59.621400 I <span class="p">|</span> etcdserver/membership: added member 8e9e05c52164694d <span class="o">[</span>http://localhost:2380<span class="o">]</span> to cluster cdf818194e3a8c32
root@controlplane:~# 
Note: In this <span class="k">case</span>, we are restoring the snapshot to a different directory but <span class="k">in</span> the same server where we took the backup <span class="o">(</span>the controlplane node<span class="o">)</span> 
As a result, the only required option <span class="k">for</span> the restore <span class="nb">command</span> is the --data-dir.


Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, 
so, the only change to be made <span class="k">in</span> the YAML file, is to change the hostPath <span class="k">for</span> the volume called etcd-data from old directory <span class="o">(</span>/var/lib/etcd<span class="o">)</span> to the new directory /var/lib/etcd-from-backup.

have to update both <span class="k">in</span> hostpaths and volumeMounts

    volumeMounts:
    - mountPath: /var/lib/etcd-from-backup
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: <span class="nb">true</span>
  priorityClassName: system-node-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
</pre></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="application-lifecycle-management.html" class="btn btn-neutral float-left" title="Application lifecycle management" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="security.html" class="btn btn-neutral float-right" title="Security" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>